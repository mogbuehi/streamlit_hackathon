# download clip to a download folder
import os
from pytube import YouTube
import gradio as gr
def download_video(url):
    try:
        yt = YouTube(url)
        video = yt.streams.get_highest_resolution()
        output_directory = './downloads'  # Note: Using './' to specify a relative path
        if not os.path.exists(output_directory):
            os.makedirs(output_directory)
        video.download(output_path=output_directory)
        return f"Video '{yt.title}' has been downloaded successfully."
    except Exception as e:
        return f"An error occurred: {e}"


interface = gr.Interface(
    fn=download_video, 
    inputs=gr.inputs.Textbox(label="Enter YouTube URL:"),
    outputs=gr.outputs.Textbox(label="Status:")
)

# Launch the interface and capture the URL
url = interface.launch()




# transcribe clip
## Convert clip to audio
from moviepy.editor import VideoFileClip
video_path = 'downloads/Kamina - Believe in me who believes in you.mp4'
video = VideoFileClip(video_path)
audio_path = f'converted_audio/{video_path[9:-4]}.wav' # removes .mp4 file extension
video.audio.write_audiofile(audio_path) 

## Transcribe audio
### Note: Thinking to run a loop to stich together overlapping segments so that transcript is clean. use prompt argument and a way to reference last x-amount of strings of transcript as its being written. 

### raw_transcript as json
import openai
import os
from dotenv import load_dotenv, find_dotenv

_ = load_dotenv(find_dotenv()) # read local .env file
openai.api_key = os.getenv('OPENAI_API_KEY')

audio_file = open(audio_path, 'rb') # rb is read binary, used to open non-txt files like audio or images
v1_transcript = openai.Audio.transcribe(
    file=audio_file,
    model="whisper-1",
    response_format='json',
    temperature=0.2
    )

### raw_transcript as vtt
audio_file = open(audio_path, 'rb') # rb is read binary, used to open non-txt files like audio or images
v1_transcript_vtt = openai.Audio.transcribe(
    file=audio_file,
    model="whisper-1",
    response_format='vtt',
    temperature=0.2
    )


# ### ask chatgpt what language this trancript is in (give it the first 1000 strs) should return a one word string
# # 
messages=[
    {'role':'system', 'content':'''You are a robot that is specifically designed to figure out what language a transcript is in. 
    When given a transcript, ONLY return the ISO-639-1 language code. Take your time and think this through.'''},
    {'role': 'user', 'content': 'je suis am√©ricain'},
    {'role': 'assistant', 'content': 'fr'},
    {'role': 'user', 'content': v1_transcript['text']}
]
response = openai.ChatCompletion.create(
    model='gpt-3.5-turbo-0613',
    messages=messages,
    temperature=0
)
language = response['choices'][0]['message']['content']
v1_transcript['language'] = language
# ### retranscribe in correct language to improve accuracy
# audio_file = open(audio_path, 'rb') # rb is read binary, used to open non-txt files like audio or images
# v2_transcript = openai.Audio.transcribe(
#     file=audio_file,
#     model="whisper-1",
#     response_format='json',
#     temperature=0.2
#     language=language
#     )
# ### 


# save transcript as jhson and or vtt file with timestamps
import json
transcript_path = f'transcript/{audio_path[15:-4]}.json'
with open(transcript_path, 'w') as json_file:
    json.dump(v1_transcript, json_file)

transcript_path = f'transcript/{audio_path[15:-4]}.vtt'
with open(transcript_path, 'w',encoding='utf-8') as vtt_file:
    vtt_file.write(v1_transcript_vtt)
    
print(v1_transcript)
print('')
print(v1_transcript_vtt)


# pass to teacher the v1_transcript (json object) or load the object
