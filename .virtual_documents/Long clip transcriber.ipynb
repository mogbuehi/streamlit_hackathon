# download clip to a download folder
import os
from pytube import YouTube
import gradio as gr
def download_video(url):
    try:
        yt = YouTube(url)
        video = yt.streams.get_highest_resolution()
        output_directory = './downloads'  # Note: Using './' to specify a relative path
        if not os.path.exists(output_directory):
            os.makedirs(output_directory)
        video.download(output_path=output_directory)
        return f"Video '{yt.title}' has been downloaded successfully."
    except Exception as e:
        return f"An error occurred: {e}"


interface = gr.Interface(
    fn=download_video, 
    inputs=gr.inputs.Textbox(label="Enter YouTube URL:"),
    outputs=gr.outputs.Textbox(label="Status:")
)

# Launch the interface and capture the URL
url = interface.launch()



# Take WAV file from downloads and save to converted audio folder
import os
from dotenv import load_dotenv, find_dotenv
import openai
import eyed3
import pyaudio
import wave
import json

# Load API key from .env file
load_dotenv(find_dotenv()) 
openai.api_key = os.getenv('OPENAI_API_KEY')

def split_audio_pyaudio(audio_path, clip_duration_sec=30, overlap_duration_sec=3, output_folder='converted_audio/clips'):
    clip_list = []
    file_name = os.path.basename(audio_path)[:-4]
    
    # Determine the total duration of the audio file
    if audio_path.endswith('.mp3'):
        audio_file = eyed3.load(audio_path)
        total_duration = audio_file.info.time_secs
    elif audio_path.endswith('.wav'):
        with wave.open(audio_path, 'rb') as wf:
            n_frames = wf.getnframes()
            framerate = wf.getframerate()
            total_duration = n_frames / framerate
    else:
        raise ValueError("Unsupported audio format")
    
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    CHUNK = 1024
    FORMAT = pyaudio.paInt16
    CHANNELS = 2
    RATE = 44100
    clip_duration_samples = clip_duration_sec * RATE
    overlap_samples = overlap_duration_sec * RATE
    
    wf = wave.open(audio_path, 'rb')
    p = pyaudio.PyAudio()

    clip_number = 1
    for start_sample in range(0, int(total_duration * RATE), clip_duration_samples - overlap_samples):
        frames = []

        for i in range(0, int(RATE / CHUNK * clip_duration_sec)):
            data = wf.readframes(CHUNK)
            if not data:
                break
            frames.append(data)

        clip_path = f"{output_folder}/{file_name}_clip_{clip_number}.wav"
        wf_clip = wave.open(clip_path, 'wb')
        wf_clip.setnchannels(CHANNELS)
        wf_clip.setsampwidth(p.get_sample_size(FORMAT))
        wf_clip.setframerate(RATE)
        wf_clip.writeframes(b''.join(frames))
        wf_clip.close()

        clip_list.append(clip_path)
        clip_number += 1

    wf.close()
    p.terminate()

    return clip_list

def transcribe(audio_path):
    clip_path_list = split_audio_pyaudio(audio_path)
    file_name = os.path.basename(audio_path)[:-4]
    transcribed_clips = []
    # Ensure the transcript file is empty to start with
    with open(f'{file_name}.txt', 'w') as txt_file:
        pass

    for clip in clip_path_list:
        with wave.open(clip, 'rb') as wf:
            n_frames = wf.getnframes()
            framerate = wf.getframerate()
            clip_duration = n_frames / framerate
            
            if clip_duration < 0.1:
                print(f"Skipping short clip (less than 0.1 sec): {clip}")
                continue  # Skip short clips
        
        with open(clip, 'rb') as audio_file:
            transcript = openai.Audio.transcribe(
                file=audio_file,
                model="whisper-1",
                response_format='text',
                temperature=0.2
            )
            transcribed_clips.append(transcript + '\n')
            # with open(f'transcript/{file_name}.text', 'a') as txt_file:
            #     txt_file.write(transcript + '\n')
    full_transcript_str = ''.join(transcribed_clips)
   
    messages=[
    {'role':'system', 'content':'''You are a robot that is specifically designed to figure out what language a transcript is in. 
    When given a transcript, ONLY return the ISO-639-1 language code. Take your time and think this through.'''},
    {'role': 'user', 'content': 'je suis amÃ©ricain'},
    {'role': 'assistant', 'content': 'fr'},
    {'role': 'user', 'content': full_transcript_str}
    ]
    
    response = openai.ChatCompletion.create(
        model='gpt-3.5-turbo-0613',
        messages=messages,
        temperature=0
    )
    language = response['choices'][0]['message']['content']
    
    transcript_json = {'transcript': full_transcript_str, 'language' : language}                   
    
    with open(f'transcript/{file_name}.json', 'w') as json_file:
        json.dump(transcript_json, json_file, indent=4)

    print("Transcription process completed!")

transcribe('simple.wav')




